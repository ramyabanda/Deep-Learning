# -*- coding: utf-8 -*-
"""hw04_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ElBABQIhr71Mc4F57FBtG2EHI5k_QMC5
"""

!nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as tvt
import numpy as np

torch.manual_seed(0)
np.random.seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

transform = tvt.Compose([tvt.Resize(244),tvt.RandomCrop(244),tvt.ToTensor(), tvt.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
dataroot_train = "/content/drive/My Drive/imagenet_images/train"
dataroot_test = "/content/drive/My Drive/imagenet_images/test"

train_data_loc = torchvision.datasets.ImageFolder(dataroot_train,transform = transform)
test_data_loc = torchvision.datasets.ImageFolder(dataroot_test,transform = transform)

trainloader = torch.utils.data.DataLoader(train_data_loc, batch_size=64,shuffle=True, num_workers=2)
testloader = torch.utils.data.DataLoader(test_data_loc, batch_size=64, shuffle=False, num_workers=2)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def prog():
  def conv3by3(in_ch, out_ch, stride=1):
    return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, groups=1, bias=False, dilation=1)

  class Block1(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, downsample=False):
      super(Block1, self).__init__()
      norm_layers = nn.BatchNorm2d
      self.conv1 = conv3by3(in_ch, out_ch, stride)
      self.bn1 = norm_layers(out_ch)
      self.relu = nn.ReLU(inplace=True)
      self.conv2 = conv3by3(out_ch, out_ch)
      self.bn2 = norm_layers(out_ch)
      self.bn3 = norm_layers(out_ch)
      if downsample == False:
        self.down_sample = None
      else:
        self.down_sample = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=2)
      self.stride = stride

    def forward(self, x):
          identity = x
          out = self.conv1(x)
          out = self.bn1(out)
          out = self.relu(out)
          out = self.conv2(out)
          out = self.bn2(out)

          if self.down_sample is not None:
              identity = self.down_sample(x)

          out += identity
          out = self.bn3(out)
          out = self.relu(out)

          return out

# This resnet architecture is similar to the standard resnet implementation retrieved from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py
  class ResNet(nn.Module):
    def __init__(self, block,  num_classes=5):
        super(ResNet, self).__init__()
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,
                                bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._do_layer(block, 64, 64, 2)
        self.layer2 = self._do_layer(block, 64, 64, 2, stride=2)
        self.layer3 = self._do_layer(block, 64, 128, 2, stride=2)
        self.layer4 = self._do_layer(block, 128, 128, 2, stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(128 , num_classes)   


    def _do_layer(self, block, in_ch, out_ch, blocks, stride=1):
   
        layers = []
        if stride == 2:
          down_sample = True
        else:
          down_sample = False
        layers.append(block(in_ch, out_ch,stride=stride, downsample = down_sample))
        for _ in range(1, blocks):
            layers.append(block(out_ch, out_ch, stride = 1, downsample= False))

        return nn.Sequential(*layers)

    def _forward_impl(self, x):

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

    def forward(self, x):
        return self._forward_impl(x)

  model = ResNet(Block1, num_classes = 5)
  epochs = 20
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  def run_code_for_training(net):
      net = net.to(device)
      criterion = torch.nn.CrossEntropyLoss()
      optimizer = torch.optim.SGD(net.parameters(), lr=1e-3, momentum=0.9)
      for epoch in range(epochs):
        running_loss = 0.0
        for i, data in enumerate(trainloader):
          inputs, labels = data
          inputs = inputs.to(device)
          labels = labels.to(device)
          optimizer.zero_grad()
          outputs = net(inputs)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
          running_loss += loss.item()
          if (i+1) % 52 == 0:
            print("[epoch:%d] loss: %.3f" % (epoch + 1, running_loss / (i+1)))
            f.write("[epoch:%d] loss: %.3f\n" % (epoch + 1, running_loss / (i+1)))

  #Testing 
  def run_code_for_testing(net):
    net = net.to(device)
    batch_size = 64
    
    correct_count, total_count, accuracy = 0,0,0
    for i, data in enumerate(testloader):
      inputs, labels = data
      inputs = inputs.to(device)
      labels = labels.to(device)
      outputs = net(inputs)
      pred_label = outputs.max(1)
      for j in range(len(pred_label)):
        if(pred_label.indices[j] == labels[j]):
          correct_count +=1 
        total_count += 1
    correct_count /= total_count
    accuracy = correct_count * 100
    print("Classification accuracy",accuracy,"%")
    f.write("Classification Accuracy :{}%".format(accuracy))

  f = open("output.txt", 'w')

  run_code_for_training(model)
  run_code_for_testing(model)

if __name__ == '__main__':
  prog()

"""##  **Variant 1**



> The general idea of Resnet architecture is similar to the standard resnet implementation retrieved from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py. In this version of the SkipConnection, a series of conv1, bn1, relu, conv2, bn2 was used in the block and then was added to another bn3 and a relu. With multiple Batch Normalizations I was hoping to prevent blowups by normalizing intermittently. The loss obtained for 20 epochs are as follows while achieving a testing accuracy of 33.33%.



```
[epoch:1] loss: 1.506
[epoch:2] loss: 1.193
[epoch:3] loss: 1.019
[epoch:4] loss: 0.909
[epoch:5] loss: 0.832
[epoch:6] loss: 0.787
[epoch:7] loss: 0.709
[epoch:8] loss: 0.676
[epoch:9] loss: 0.659
[epoch:10] loss: 0.595
[epoch:11] loss: 0.585
[epoch:12] loss: 0.549
[epoch:13] loss: 0.525
[epoch:14] loss: 0.500
[epoch:15] loss: 0.514
[epoch:16] loss: 0.450
[epoch:17] loss: 0.442
[epoch:18] loss: 0.410
[epoch:19] loss: 0.412
[epoch:20] loss: 0.351
Classification Accuracy :33.33333333333333%
```


> The SkipConnections block for the above code was as follows:

```
  class Block1(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, downsample=False):
      super(Block1, self).__init__()
    
      norm_layers = nn.BatchNorm2d
      self.conv1 = conv3by3(in_ch, out_ch, stride)
      self.bn1 = norm_layers(out_ch)
      self.relu = nn.ReLU(inplace=True)
      self.conv2 = conv3by3(out_ch, out_ch)
      self.bn2 = norm_layers(out_ch)
      self.bn3 = norm_layers(out_ch)
      if downsample == False:
        self.down_sample = None
      else:
        self.down_sample = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=2)
      self.stride = stride

    def forward(self, x):
          identity = x
          out = self.conv1(x)
          out = self.bn1(out)
          out = self.relu(out)
          out = self.conv2(out)
          out = self.bn2(out)

          if self.down_sample is not None:
              identity = self.down_sample(x)

          out += identity
          out = self.bn3(out)
          out = self.relu(out)

          return out
```

##  **Variant 2**



> In this version of the SkipConnections block, I tried to keep it simple by having a series of Conv1, bn1, Relu in side the skip block, and the same series again outside the skip block. The loss obtained for 20 epochs are as follows while achieving a testing accuracy of 30.55%.



```
[epoch:1] loss: 1.488
[epoch:2] loss: 1.132
[epoch:3] loss: 0.986
[epoch:4] loss: 0.851
[epoch:5] loss: 0.810
[epoch:6] loss: 0.762
[epoch:7] loss: 0.678
[epoch:8] loss: 0.671
[epoch:9] loss: 0.633
[epoch:10] loss: 0.584
[epoch:11] loss: 0.565
[epoch:12] loss: 0.543
[epoch:13] loss: 0.518
[epoch:14] loss: 0.485
[epoch:15] loss: 0.497
[epoch:16] loss: 0.436
[epoch:17] loss: 0.424
[epoch:18] loss: 0.411
[epoch:19] loss: 0.385
[epoch:20] loss: 0.359
Classification accuracy 30.555555555555557 %
```

> The SkipConnections block for the above code was as follows:



```
  class Block(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, downsample=False):
      super(Block, self).__init__()
    
      norm_layers = nn.BatchNorm2d
      self.conv1 = conv3by3(in_ch, out_ch, stride)
      self.bn1 = norm_layers(out_ch)
      self.relu = nn.ReLU(inplace=True)
      self.conv2 = conv3by3(out_ch, out_ch)
      self.bn2 = norm_layers(out_ch)
      if downsample == False:
        self.down_sample = None
      else:
        self.down_sample = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=2)
      self.stride = stride

    def forward(self, x):
          identity = x
          out = self.conv1(x)
          out = self.bn1(out)
          out = self.relu(out)

          if self.down_sample is not None:
              identity = self.down_sample(x)

          out = out.clone() + identity
          out = self.conv2(out)
          out = self.bn2(out)
          out = self.relu(out)

          return out
```

## **Variant 3**



> In this version of the SkipConnections, it was similar to the previous in number of units for each conv, bn, relu but the difference was that a bn2, relu were outside the skip block. The loss obtained for 20 epochs are as follows while achieving a testing accuracy of 29.16%.



```
[epoch:1] loss: 1.530
[epoch:2] loss: 1.272
[epoch:3] loss: 1.111
[epoch:4] loss: 0.978
[epoch:5] loss: 0.895
[epoch:6] loss: 0.808
[epoch:7] loss: 0.767
[epoch:8] loss: 0.734
[epoch:9] loss: 0.697
[epoch:10] loss: 0.664
[epoch:11] loss: 0.648
[epoch:12] loss: 0.622
[epoch:13] loss: 0.615
[epoch:14] loss: 0.572
[epoch:15] loss: 0.565
[epoch:16] loss: 0.536
[epoch:17] loss: 0.494
[epoch:18] loss: 0.476
[epoch:19] loss: 0.470
[epoch:20] loss: 0.472
Classification accuracy 29.166666666666668 %
```


> The SkipConnections block for the above code was as follows:



```
  class Block2(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, downsample=False):
      super(Block2, self).__init__()
    
      norm_layers = nn.BatchNorm2d
      self.conv1 = conv3by3(in_ch, out_ch, stride)
      self.bn1 = norm_layers(out_ch)
      self.relu = nn.ReLU(inplace=True)
      self.conv2 = conv3by3(out_ch, out_ch)
      self.bn2 = norm_layers(out_ch)
      self.bn3 = norm_layers(out_ch)
      self.conv3 = conv3by3(out_ch, out_ch)
      if downsample == False:
        self.down_sample = None
      else:
        self.down_sample = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=2)
      self.stride = stride

    def forward(self, x):
          identity = x
          out = self.conv1(x)
          out = self.bn1(out)
          out = self.relu(out)
          out = self.conv2(out)
          

          if self.down_sample is not None:
              identity = self.down_sample(x)

          out += identity
          out = self.bn2(out)
          out = self.relu(out)

          return out
```

## **Variant 4**



> In this version of the SkipConnection, I started my skip block with a ReLU and continued with the usual series of conv, bn, relu and added another conv before ending the block and having a bn2 and relu outside my block. The loss obtained for 20 epochs are as follows while achieving a testing accuracy of 27.77%.



```
[epoch:1] loss: 1.542
[epoch:2] loss: 1.282
[epoch:3] loss: 1.085
[epoch:4] loss: 0.975
[epoch:5] loss: 0.889
[epoch:6] loss: 0.834
[epoch:7] loss: 0.779
[epoch:8] loss: 0.748
[epoch:9] loss: 0.679
[epoch:10] loss: 0.669
[epoch:11] loss: 0.638
[epoch:12] loss: 0.626
[epoch:13] loss: 0.583
[epoch:14] loss: 0.549
[epoch:15] loss: 0.544
[epoch:16] loss: 0.513
[epoch:17] loss: 0.494
[epoch:18] loss: 0.500
[epoch:19] loss: 0.452
[epoch:20] loss: 0.430
27.77777777777778 %
```
> The SkipConnections block for the above code was as follows:


```
class Block3(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, downsample=False):
      super(Block3, self).__init__()
    
      norm_layers = nn.BatchNorm2d
      self.conv1 = conv3by3(in_ch, out_ch, stride)
      self.bn1 = norm_layers(out_ch)
      self.relu = nn.ReLU(inplace=False)
      self.conv2 = conv3by3(out_ch, out_ch)
      self.bn2 = norm_layers(out_ch)
      self.bn3 = norm_layers(out_ch)
      self.conv3 = conv3by3(out_ch, out_ch)
      if downsample == False:
        self.down_sample = None
      else:
        self.down_sample = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=2)
      self.stride = stride

    def forward(self, x):
          identity = x
          out = self.relu(x)
          out = self.conv1(out)
          out = self.bn1(out)
          out = self.relu(out)
          out = self.conv2(out)

          if self.down_sample is not None:
              identity = self.down_sample(x)

          out = out.clone() + identity
          out = self.bn2(out)
          out = self.relu(out)

          return out
```

## **Variant 5**



> In this version of the SkipConnection block, it included a series of conv, batch normalization, relu occur twice before ending the block. There also does not exist any other units ouside this block. The loss obtained for 20 epochs are as follows while achieving a testing accuracy of 29.16%.

```
[epoch:1] loss: 1.383
[epoch:2] loss: 1.035
[epoch:3] loss: 0.898
[epoch:4] loss: 0.813
[epoch:5] loss: 0.748
[epoch:6] loss: 0.696
[epoch:7] loss: 0.686
[epoch:8] loss: 0.649
[epoch:9] loss: 0.588
[epoch:10] loss: 0.566
[epoch:11] loss: 0.589
[epoch:12] loss: 0.549
[epoch:13] loss: 0.505
[epoch:14] loss: 0.485
[epoch:15] loss: 0.474
[epoch:16] loss: 0.449
[epoch:17] loss: 0.418
[epoch:18] loss: 0.413
[epoch:19] loss: 0.388
[epoch:20] loss: 0.370
Classification accuracy 29.166666666666668 %
```


> The SkipConnections block for the above code was as follows:



```
  class Block4(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, downsample=False):
      super(Block4, self).__init__()
    
      norm_layers = nn.BatchNorm2d
      self.conv1 = conv3by3(in_ch, out_ch, stride)
      self.bn1 = norm_layers(out_ch)
      self.relu = nn.ReLU(inplace=True)
      self.conv2 = conv3by3(out_ch, out_ch)
      self.bn2 = norm_layers(out_ch)
      self.bn3 = norm_layers(out_ch)
      if downsample == False:
        self.down_sample = None
      else:
        self.down_sample = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=2)
      self.stride = stride

    def forward(self, x):
          identity = x
          out = self.conv1(x)
          out = self.bn1(out)
          out = self.relu(out)
          out = self.conv2(out)
          out = self.bn2(out)
          out = self.relu(out)
          
          if self.down_sample is not None:
              identity = self.down_sample(x)

          out = out.clone() + identity

          return out
```
"""