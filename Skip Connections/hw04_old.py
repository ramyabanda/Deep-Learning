# -*- coding: utf-8 -*-
"""hw04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-veeKLsHagtu-OIcX_DyKWfrSeg6VHvP
"""

!nvidia-smi

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as tvt
import numpy as np

torch.manual_seed(0)
np.random.seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

transform = tvt.Compose([tvt.ToTensor(), tvt.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
dataroot = "Users/ramyabanda/Downloads/ImageNet-Datasets-Downloader-master"

train_data_loc = torchvision.datasets.CIFAR10(root=dataroot, train=True,download=True, transform=transform)
test_data_loc = torchvision.datasets.CIFAR10(root=dataroot, train=False,download=True,transform=transform)

trainloader = torch.utils.data.DataLoader(train_data_loc, batch_size=256,shuffle=True, num_workers=2)
testloader = torch.utils.data.DataLoader(test_data_loc, batch_size=256, shuffle=False, num_workers=2)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def prog():
  def conv3by3(in_ch, out_ch, stride=1):
    return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, groups=1, bias=False, dilation=1)

  class Block(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, downsample=False):
      super(Block, self).__init__()
    
      norm_layers = nn.BatchNorm2d
      self.conv1 = conv3by3(in_ch, out_ch, stride)
      self.bn1 = norm_layers(out_ch)
      self.relu = nn.ReLU(inplace=True)
      self.conv2 = conv3by3(out_ch, out_ch)
      self.bn2 = norm_layers(out_ch)
      if downsample == False:
        self.down_sample = None
      else:
        self.down_sample = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=2)
      self.stride = stride

    def forward(self, x):
          identity = x
          out = self.conv1(x)
          out = self.bn1(out)
          out = self.relu(out)
          out = self.conv2(out)
          out = self.bn2(out)

          if self.down_sample is not None:
              identity = self.down_sample(x)

          out += identity
          out = self.relu(out)

          return out

# this resnet architecture is similar to the standard resnet implementation retrieved from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py
  class ResNet(nn.Module):
    def __init__(self, block,  num_classes=5):
        super(ResNet, self).__init__()
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,
                                bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._do_layer(block, 64, 64, 4)
        self.layer2 = self._do_layer(block, 64, 128, 4, stride=2)
        self.layer3 = self._do_layer(block, 128, 256, 4, stride=2)
        self.layer4 = self._do_layer(block, 256, 512, 4, stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 , num_classes)

        

    def _do_layer(self, block, in_ch, out_ch, blocks, stride=1):
   
        layers = []
        if stride == 2:
          down_sample = True
        else:
          down_sample = False
        layers.append(block(in_ch, out_ch,stride=stride, downsample = down_sample))
        for _ in range(1, blocks):
            layers.append(block(out_ch, out_ch, stride = 1, downsample= False))

        return nn.Sequential(*layers)

    def _forward_impl(self, x):
        # See note [TorchScript super()]
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

    def forward(self, x):
        return self._forward_impl(x)

  model = ResNet(Block, num_classes = 10)
  epochs = 10
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  def run_code_for_training(net):
      net = net.to(device)
      criterion = torch.nn.CrossEntropyLoss()
      optimizer = torch.optim.SGD(net.parameters(), lr=1e-3, momentum=0.9)
      for epoch in range(epochs):
        running_loss = 0.0
        for i, data in enumerate(trainloader):
          inputs, labels = data
          inputs = inputs.to(device)
          labels = labels.to(device)
          optimizer.zero_grad()
          outputs = net(inputs)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()
          running_loss += loss.item()
          if (i+1) % 196 == 0:
            print("[epoch:%d] loss: %.3f" % (epoch + 1, running_loss / (i+1)))
            f.write("[epoch:%d] loss: %.3f\n" % (epoch + 1, running_loss / (i+1)))

  #Testing and Confusion Matrix
  def run_code_for_testing(net):
    net = net.to(device)
    batch_size = 256
    
    correct_count, total_count, accuracy = 0,0,0
    for i, data in enumerate(testloader):
      inputs, labels = data
      inputs = inputs.to(device)
      labels = labels.to(device)
      outputs = net(inputs)
      pred_label = outputs.max(1)
      for j in range(len(pred_label)):
        if(pred_label.indices[j] == labels[j]):
          correct_count +=1 
        total_count += 1
    correct_count /= total_count
    accuracy = correct_count * 100
    print(accuracy,"%")
    f.write("Classification Accuracy :{}%".format(accuracy))

  f = open("output.txt", 'w')

  run_code_for_training(model)
  run_code_for_testing(model)

#general idea of the block; it's loss or accuracy
class Block_0(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, downsample=None):
      super(Block_0, self).__init__()
    
      norm_layers = nn.BatchNorm2d
      self.conv1 = conv3by3(in_ch, out_ch, stride)
      self.bn1 = norm_layers(out_ch)
      self.relu = nn.ReLU(inplace=True)
      self.conv2 = conv3by3(out_ch, out_ch)
      self.bn2 = norm_layers(out_ch)
      self.downsample = downsample
      self.stride = stride

    def forward(self, x):
          identity = x
          out = self.conv1(x)
          out = self.bn1(out)
          out = self.relu(out)
          out = self.conv2(out)
          out = self.bn2(out)

          if self.downsample is not None:
              identity = self.downsample(x)

          out += identity
          out = self.relu(out)

          return out

if __name__ == '__main__':
  prog()